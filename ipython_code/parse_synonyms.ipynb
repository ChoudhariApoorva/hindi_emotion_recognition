{
 "metadata": {
  "name": "parse_synonyms"
 }, 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import csv", 
      "import os", 
      "#from urllib.request import urlopen", 
      "from urllib2 import urlopen", 
      "import codecs", 
      "import urllib2", 
      "", 
      "", 
      "BASE_PATH = \"/home/sunil/nlp_project/\"", 
      "ANEW_PATH = os.path.join(BASE_PATH, \"raw/anew/all.csv\")", 
      "PARSED_PATH = os.path.join(BASE_PATH, \"raw/\")", 
      "print PARSED_PATH", 
      "", 
      "", 
      "anew_data = dict()", 
      "with open(ANEW_PATH, \"r\") as file_pointer:", 
      "    line = csv.reader(file_pointer, delimiter=',')", 
      "    for row in line:", 
      "        if row[0] not in anew_data:", 
      "            anew_data[row[0]] = dict()", 
      "        anew_data[row[0]][\"word\"] = row[0]", 
      "        anew_data[row[0]][\"word_num\"] = row[1]", 
      "        anew_data[row[0]][\"valence_mean\"] = row[2]", 
      "        anew_data[row[0]][\"valence_sd\"] = row[3]", 
      "        anew_data[row[0]][\"arousal_mean\"] = row[4]", 
      "        anew_data[row[0]][\"arousal_sd\"] = row[5]", 
      "        anew_data[row[0]][\"dominance_mean\"] = row[6]", 
      "        anew_data[row[0]][\"dominance_sd\"] = row[7]", 
      "        anew_data[row[0]][\"word_frequency\"] = row[8]", 
      "", 
      "", 
      "generate_url = list()", 
      "for key in anew_data.keys():", 
      "    generate_url.append(key)", 
      "", 
      "", 
      "from glob import glob", 
      "", 
      "[\"http://www.shabdkosh.com/hi/translate?e=\"+ key +\"&l=hi\", key]", 
      "", 
      "downloaded_files = glob(PARSED_PATH + \"*.txt\")", 
      "generate_files = {PARSED_PATH + x +\"_hindi.txt\": x for x in generate_url}", 
      "#print len(generate_files)", 
      "", 
      "to_download_set = list(set(generate_files.keys()) - set(downloaded_files))", 
      "#print len(to_download_set)", 
      "", 
      "to_download_list = list()", 
      "for val in to_download_set:", 
      "    to_download_list.append([\"http://www.shabdkosh.com/hi/translate?e=\"+ \\", 
      "                            generate_files[val] +\"&l=hi\", generate_files[val]])", 
      "print len(to_download_list)   "
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "/home/sunil/nlp_project/raw/", 
        "386"
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        ""
       ]
      }
     ], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": [
      "import urllib2", 
      "from time import sleep", 
      "import re", 
      "", 
      "request_headers = {", 
      "\"Accept-Language\": \"en-US,en;q=0.5\",", 
      "\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:40.0) Gecko/20100101 Firefox/40.0\",", 
      "\"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",", 
      "\"Referer\": \"http://thewebsite.com\",", 
      "\"Connection\": \"keep-alive\" ", 
      "}", 
      "", 
      "sorry_word = re.compile(\"Sorry\")", 
      "", 
      "count = 0", 
      "for url in to_download_list:", 
      "    try:", 
      "        print url[0]", 
      "        request = urllib2.Request(url[0], headers=request_headers)", 
      "        contents = urllib2.urlopen(request).read()", 
      "        if sorry_word.search(contents):            ", 
      "            with open('sorry_found.txt','a') as f:", 
      "                f.write('\\n'+ url[0])", 
      "            sleep(30)", 
      "            count = count + 1", 
      "            if count > 10:", 
      "                break", 
      "            continue", 
      "        count = 0", 
      "        with codecs.open(PARSED_PATH + \"/\" + url[1]+\"_hindi.txt\", \"wb\", encoding=\"utf-8\") as file_pointer:", 
      "            file_pointer.write(contents.decode('utf-8'))", 
      "        sleep(30)", 
      "    except urllib2.HTTPError, e:", 
      "        print e.code", 
      "    except urllib2.URLError, e:", 
      "        print e.args"
     ], 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "http://www.shabdkosh.com/hi/translate?e=perfume&l=hi", 
        "http://www.shabdkosh.com/hi/translate?e=troubled&l=hi"
       ]
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": [
        "", 
        "http://www.shabdkosh.com/hi/translate?e=skijump&l=hi"
       ]
      }
     ], 
     "prompt_number": "*"
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": [], 
     "language": "python", 
     "outputs": []
    }
   ]
  }
 ]
}